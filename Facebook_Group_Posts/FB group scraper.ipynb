{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from facepy import GraphAPI\n",
    "import spacy\n",
    "from textacy import extract\n",
    "from textacy.doc import Doc\n",
    "from textacy import similarity\n",
    "\n",
    "stdout = sys.stdout\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')\n",
    "sys.stdout = stdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this post we will use modern NLP techniques to find similar posts in a facebook group. \n",
    "\n",
    "You have probably been in a situation where you want to post something in a facebook group and you are not sure whether almost the same post already exists and maybe just hiding on the next page. You could click through the pages and read all the posts or you could use the search but both ways have their disadvantages. Where the first one is obviosly very tidious, for the second one you will have to try different words and phrasing in order not to miss something.\n",
    "\n",
    "Out goal here is to find the most similar posts in a group based on their meaning.\n",
    "\n",
    "First you will need to get an `App-ID` and `App-Secret` from facebook in order to access the content through the API. It is very simple and you can find the required steps here https://developers.facebook.com/docs/apps/register\n",
    "\n",
    "Second you will need the `Group-ID`. I decided to look at the posts in the group \"Python Programming Language\" - Group  https://www.facebook.com/groups/python.programmers/. The easiest way to get the ID is to go to tha website https://lookup-id.com and post the URL of the Group there.\n",
    "\n",
    "We will use the library facepy (https://github.com/jgorset/facepy) which \"makes it really easy to use Facebook's Graph API\". You can install `facepy` with `pip install facepy`\n",
    "\n",
    "Once you have installed facepy and have got your App-ID, App-Secret and Group-ID, scraping all posts from the group wall can be done with just a couple lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading page 0\n",
      "Downloading page 10\n",
      "Downloading page 20\n",
      "Downloading page 30\n",
      "Downloading page 40\n",
      "Downloading page 50\n",
      "Downloading page 60\n",
      "Downloading page 70\n"
     ]
    }
   ],
   "source": [
    "group_id = '457660044251817' #https://www.facebook.com/groups/python.programmers/\n",
    "app_id = os.environ[\"FB_APP_ID\"]\n",
    "app_secret = os.environ[\"FB_APP_SECRET\"]\n",
    "access_token = app_id + \"|\" + app_secret\n",
    "\n",
    "graph = GraphAPI(access_token)\n",
    "pages = graph.get(group_id + \"/feed\", page=True, retry=3, limit=100)\n",
    "i = 0\n",
    "posts =[]\n",
    "\n",
    "#Iterate through pages and store all posts in one list\n",
    "for p in pages:\n",
    "    if i%10 == 0:\n",
    "        print 'Downloading page', i\n",
    "    posts.extend(p['data'])\n",
    "    i += 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the list to a dataframe for more comfort\n",
    "df = pd.DataFrame(posts)\n",
    "\n",
    "#Drop empty posts\n",
    "df.dropna(subset=['message'],inplace=True)\n",
    "\n",
    "#Do some cleaning and drop posts posts with less than 20 characters\n",
    "df[\"cleanmessage\"] = (df['message'].str.lower()\n",
    "                                   .str.replace(r'\\n|\\r|\\t', ' ')\n",
    "                                   .str.replace(r'[^a-z0-9+]+', ' ')\n",
    "                                   .str.replace(r' +', ' '))\n",
    "\n",
    "df = df.loc[df['cleanmessage'].str.len() > 20]\n",
    "df.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many different ways to calculate the similarity between two documents. First we need to decide how do we want to represent the document (Bag of words https://en.wikipedia.org/wiki/Bag-of-words_model, Tf-Idf https://en.wikipedia.org/wiki/Tf%E2%80%93idf, word embeddings https://en.wikipedia.org/wiki/Word_embedding) and then we have to choose the distance metric (euclidean distance https://en.wikipedia.org/wiki/Euclidean_distance, cosine similarity https://en.wikipedia.org/wiki/Cosine_similarity, word mover's distance http://proceedings.mlr.press/v37/kusnerb15.pdf) which will tell us how close (similar) are two documents.\n",
    "\n",
    "We are going to represent the content of the facebook posts with word embeddings and compare the transformed posts with word mover's distance. The combination of both have shown lower k-nearest neighbor document\n",
    "classification error rates compared to other state of the art techniques.\n",
    "\n",
    "The advantage of word embeddings is that the words which have similar meanings but donâ€™t have any letters in common will still have similar vectors (be close) in the embedded space (e.g. `lion` and `tiger` ). \n",
    "\n",
    "This requires a model that has been trained on a large corpus of text of the respective language. Luckliy for us such models are ready available and we don't have to train our own. We will use the library `spaCy` (https://spacy.io/) to transform the the documents. You can find the instruction on how to install `spaCy` and how to download the language models here https://spacy.io/docs/usage/. Currently four languages ( `EN` , `DE` , `ES` , `FR` ) are supported out of the box but you can find even more open sourced language models and add them to library yourself. \n",
    "\n",
    "Additionally we will use the library `textacy` (http://textacy.readthedocs.io/en/latest/) which is build on top of `spacy` to compute the word mover's distance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load spaCy's english model\n",
    "nlp = spacy.load('EN')\n",
    "\n",
    "textacy_docs = {}\n",
    "\n",
    "#Transform the posts to spaCy documents\n",
    "docs = list(nlp.pipe(df.cleanmessage.values, \n",
    "                      batch_size=1000, \n",
    "                      n_threads=3, \n",
    "                      tag=False, \n",
    "                      entity=False))\n",
    "\n",
    "#Transform spaCy documents to textacy documents\n",
    "for i in np.arange(len(docs)):\n",
    "    try:\n",
    "        textacy_docs[i] = Doc(docs[i])\n",
    "    except Exception as e:\n",
    "        print(\"Failed to get word vector:{}, {}\".format(i, e))\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create a textacy document from your own post\n",
    "my_post = unicode(\"\"\"\n",
    "I want to become a data scientist.\n",
    "What online resources can you recommend? \n",
    "\"\"\")\n",
    "\n",
    "clean_post = (my_post.lower().replace(r'\\n|\\r|\\t', ' ')\n",
    "                          .replace(r'[^a-z0-9+]+', ' ') \n",
    "                          .replace(r' +', ' '))\n",
    "\n",
    "doc2 = Doc(content=clean_post, lang=nlp.lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def has_vector(doc):\n",
    "    t = list(extract.words(doc))\n",
    "    has_vec = np.sum([x.has_vector for x in t])\n",
    "    return (has_vec > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iterate through the posts and calculate the similarity between your own post and the existing posts\n",
    "similarities = {}\n",
    "for i, doc1 in textacy_docs.iteritems():\n",
    "    #check if has any word vectors\n",
    "    if (not has_vector(doc1)):\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        this_sim = similarity.word_movers(doc1, doc2)\n",
    "        if not np.isnan(this_sim):\n",
    "            similarities[i] = this_sim\n",
    "    except Exception as e:\n",
    "        print e,i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My post:\n",
      " \n",
      "I want to become a data scientist.\n",
      "What online resources can you recommend? \n",
      "\n",
      "\n",
      "\n",
      "Most relevant previous posts\n",
      "\n",
      "1) What exactly  statistical knowledge I need to acquire for data science,machine learning. Predictive modelling etc??\n",
      "\n",
      "Want answers from experienced data scientists.\n",
      "https://www.facebook.com/groups/457660044251817/1016982104986272\n",
      "\n",
      "2) What to become an expert in data science by learning some set of online coursers targeted to help you truly master your data science skill. Then this month is the great way start. \n",
      "coursera offering different data science specializations to full fill your dream to become data science expert. Have look at coursera data science specializations list ordered by DataAspirant team. \n",
      "ALL THE BEST\n",
      "\n",
      "\n",
      "https://www.facebook.com/groups/457660044251817/1105549306129551\n",
      "\n",
      "3) 12 Python Resources for Data Science\n",
      "https://www.facebook.com/groups/457660044251817/1541228582561619\n",
      "\n",
      "4) https://datapyr.zeef.com/kranthi.kumar\n",
      "Hello everyone, I have collected some useful resources in Data science, Python and R, which I found useful, I am looking forward to adding more resources. I would appreciate it if everyone could share their valuable data science learning resources with every learner and enthusiast.\n",
      "https://www.facebook.com/groups/457660044251817/1347399138611232\n",
      "\n",
      "5) We are so excited to interview Kai Xin Thia ( Co-Founder of DataScience SG the largest data science community in Singapore) as the first data scientist for our DataAspirant blog lovers. He has shared some interesting things about data science which every data science lover has to know.\n",
      "\n",
      "https://www.facebook.com/groups/457660044251817/1015222821828867\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Find the k most similar posts and print the text and the direkt link to the post\n",
    "k = 5\n",
    "kNN = sorted(similarities, key=similarities.get)[-k:]\n",
    "\n",
    "print 'My post:\\n {}\\n\\n'.format(my_post)\n",
    "print 'Most relevant previous posts\\n'\n",
    "i = 1\n",
    "for ind, row in df.loc[kNN].iterrows():\n",
    "    print '{}) {}'.format(i,row['message'])\n",
    "    post_id = row['id'].replace('_','/')\n",
    "    print 'https://www.facebook.com/groups/{}\\n'.format(post_id)\n",
    "    i+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
